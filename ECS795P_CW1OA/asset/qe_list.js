var qe_list = [
    {
      "idx": "1.1",
      "Q/E": "Question",
      "Description": "What is the concept of single-head self-attention?"
    },
    {
      "idx": "1.2",
      "Q/E": "Question",
      "Description": "What is the concept of multi-head self-attention?"
    },
    {
      "idx": "1.3",
      "Q/E": "Question",
      "Description": "What is the difference between single-head and multi-head self-attention?"
    },
    {
      "idx": "2.1",
      "Q/E": "Question",
      "Description": "What are the main compositional layers of a transformer?"
    },
    {
      "idx": "2.2",
      "Q/E": "Question",
      "Description": "What is the purpose of layer normalization in a transformer?"
    },
    {
      "idx": "2.3",
      "Q/E": "Question",
      "Description": "What is the difference between the training and testing stage of a transformer?"
    },
    {
      "idx": "2.4",
      "Q/E": "Question",
      "Description": "What is the skip connection in a transformer?"
    },
    {
      "idx": "2.5",
      "Q/E": "Question",
      "Description": "Why does a transformer usually use multi-head attention instead of single head one?"
    },
    {
      "idx": "3.1",
      "Q/E": "Question",
      "Description": "How to use a trained Transformer to perform image classification (testing stage)?"
    },
    {
      "idx": "3.2",
      "Q/E": "Question",
      "Description": "What are the input and the output of the Transformer?"
    },
    {
      "idx": "3.3",
      "Q/E": "Question",
      "Description": "What augmentations are used for training data?"
    },
    {
      "idx": "3.4",
      "Q/E": "Question",
      "Description": "What’s the difference between the preprocess of the train set and the validation set?"
    },
    {
      "idx": "3.5",
      "Q/E": "Question",
      "Description": "Why augmentations for training data is necessary?"
    },
    {
      "idx": "1.1",
      "Q/E": "Exercise",
      "Description": "Calculate the dimensions of W<sub>q</sub> ,W<sub>v</sub> , W<sub>k</sub> in “Single-head Attention” section."
    },
    {
      "idx": "1.2",
      "Q/E": "Exercise",
      "Description": "Calculate the dimensions of W<sub>q</sub> ,W<sub>v</sub> , W<sub>k</sub> in “Multi-head Attention” section."
    },
    {
      "idx": "1.3",
      "Q/E": "Exercise",
      "Description": "Show the weight of the causal attention mask in “Causal attention mask” section."
    },
    {
      "idx": "1.4",
      "Q/E": "Exercise",
      "Description": "Show the figure of the causal attention mask in “Causal attention mask” section."
    },
    {
      "idx": "2.1",
      "Q/E": "Exercise",
      "Description": "To show the projection weight (W<sub>K</sub>) of K in the multi-head attention layer  (Tip: check the PyTorch build-in class nn.MultiheadAttention)."
    },
    {
      "idx": "2.2",
      "Q/E": "Exercise",
      "Description": "To show the projection weight (W<sub>Q</sub>) of Q in the multi-head attention layer  (Tip: check the PyTorch build-in class nn.MultiheadAttention)."
    },
    {
      "idx": "2.3",
      "Q/E": "Exercise",
      "Description": "To show the projection weight (W<sub>V</sub>) of V in the multi-head attention layer  (Tip: check the PyTorch build-in class nn.MultiheadAttention)."
    },
    {
      "idx": "3.1",
      "Q/E": "Exercise",
      "Description": "To load and preprocess the train set and validation set from the CIFAR10 dataset."
    },
    {
      "idx": "3.2",
      "Q/E": "Exercise",
      "Description": "To transform training images to patch embedding."
    },
    {
      "idx": "3.3",
      "Q/E": "Exercise",
      "Description": "To add a class toke to the patch embedding."
    },
    {
      "idx": "3.4",
      "Q/E": "Exercise",
      "Description": "To feed the input image embedding into the Transformer."
    },
    {
      "idx": "3.5",
      "Q/E": "Exercise",
      "Description": "To plot the training loss curve to show its variation with the epoch."
    },
    {
      "idx": "3.6",
      "Q/E": "Exercise",
      "Description": "To plot the accuracy score curve to show its variation with the epoch."
    },
  ]