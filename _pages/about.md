---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Currently, I am a third-year PhD student in Computer Science at the Computer Vision Group in the School of Electronic Engineering and Computer Science at the Queen Mary University of London, under the supervision of [Prof. Shaogang Gong](http://www.eecs.qmul.ac.uk/~sgg/).

Before joining QMUL, I earned my Bachelor‚Äôs degree at Communication University of China and my Master‚Äôs degree at University College London.

My current research interests revolve around Deep Learning and Computer Vision, with a particular focus on Multimodal Video Understanding and Self-supervised Learning.


# üî• News
- *2025.04*: &nbsp;üéâüéâ Our **INT** is accepted to **IJCAI'25**!
- *2024.07*: &nbsp;üéâüéâ One paper is accepted to **ECCV'24**!

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv 2025</div><img src='images/cos.jpg' alt="Arxiv2025" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CoS: Chain-of-Shot Prompting for Long Video Understanding](https://arxiv.org/abs/2502.06428), **Arxiv 2025**

Jian Hu, **Zixu Cheng**, Chenyang Si, Wei Li, Shaogang Gong

[**Paper**](https://www.ijcai.org/proceedings/2025/0124.pdf)**|**[**ArXiv**](https://arxiv.org/abs/2407.05118v1) **|**[**Code**](https://github.com/lwpyh/CoS_codes)
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->

- Long-video understanding by visual prompt learning
- Training-free mosaicing binary coding with pseudo-temporal grounding for long video understanding

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2025</div><img src='images/int.jpg' alt="IJCAI2025" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation](https://arxiv.org/abs/2407.05118v1), **IJCAI 2024*5

Jian Hu, **Zixu Cheng**, Shaogang Gong

[**Paper**](https://www.ijcai.org/proceedings/2025/0124.pdf)**ÔΩú**[**ArXiv**](https://arxiv.org/abs/2407.05118v1)
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->

- Training-free test-time adaptation for task-generic promptable segmentation
- Progressive negative mining identifies hard-to-distinguish error categories

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/CFSR.jpg' alt="ECCV2024" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding](https://arxiv.org/abs/2407.05118v1), **ECCV 2024**

**Zixu Cheng***, Yujiang Pu\*, Shaogang Gong, Parisa Kordjamshidi, Yu Kong (\*equal contribution)

[**Paper**](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02948.pdf)**ÔΩú**[**ArXiv**](https://arxiv.org/abs/2407.05118v2)**|** [**Code**](https://github.com/zxccade/SHINE) 
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->

- LLM-driven methods for hard negative generation
- Coarse-to-Fine Saliency Ranking for Compositional Temporal Grounding
</div></div>


# üìñ Educations
- *2023.09 - now*, PhD, Queen Mary University of London (QMUL), London.
- *2021.09 - 2022.09*, Postgraduate, University College London (UCL), London.
- *2016.09 - 2020.06*, Undergraduate, Communication University of China (CUC), Beijing.

